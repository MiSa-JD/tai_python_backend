{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2fadd8",
   "metadata": {},
   "source": [
    "# 실제 로직 테스트 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbddf68c",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c09d2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환경 변수 로드\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ad86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 클래스 목록\n",
    "from dataclasses import dataclass\n",
    "\n",
    "## 검색될 내용\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "  keyword: str\n",
    "  title: str\n",
    "  link: str\n",
    "  content: str\n",
    "\n",
    "## VDB에 들어갈 메타 데이터\n",
    "@dataclass\n",
    "class Metadata:\n",
    "  keyword: str\n",
    "  link: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb06abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "# 분석 모델\n",
    "llm_analize = ChatUpstage(\n",
    "    model=\"solar-mini\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# 검증 모델\n",
    "llm_validate = ChatUpstage(\n",
    "    model=\"solar-pro2\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "llm_summarize = ChatUpstage(\n",
    "    model=\"solar-mini\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "llm_classify = ChatUpstage(\n",
    "    model=\"solar-mini\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c5de9",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 사전 요소 정의 \n",
    "### 1. RAG 기능 쪽 변수 및 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "241f3e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# 스플리터 정의\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=300,\n",
    "  chunk_overlap=50\n",
    ")\n",
    "\n",
    "# 임베딩 모델 설정\n",
    "vstore = Chroma(\n",
    "  collection_name=\"knowledge_base\",\n",
    "  embedding_function=UpstageEmbeddings(model=\"embedding-query\"),\n",
    "  persist_directory=\"./chroma\"\n",
    ")\n",
    "\n",
    "# 검색자\n",
    "retriever=vstore.as_retriever(\n",
    "  search_type=\"mmr\",\n",
    "  search_kwargs={\n",
    "    \"k\":4,\n",
    "    \"fetch_k\":20,\n",
    "    \"lambda_mult\":0.5\n",
    "  }\n",
    ")\n",
    "\n",
    "# 연관 되어있는 것으로 볼 범위\n",
    "THRESHOLD = 1.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca8a51bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# VDB에 저장된 문서 묶음 검색해오기\n",
    "def search_documents(keyword: str) -> list[Document]:\n",
    "  scored = vstore.similarity_search_with_score(keyword, k=20)\n",
    "\n",
    "  filtered =  [\n",
    "    doc for doc, score in scored if score <= THRESHOLD\n",
    "  ]\n",
    "  return filtered\n",
    "\n",
    "# 검색 후 프롬프트도 만들기\n",
    "def get_prompt_on_retrieve(keyword: str, prompt: str) -> str:\n",
    "  search_results = search_documents(keyword=keyword)\n",
    "\n",
    "  # 프롬프트에 내용 삽입하기\n",
    "  result = prompt\n",
    "  for items in search_results:\n",
    "     result += (\"\\n\" + items.page_content)\n",
    "\n",
    "  return result\n",
    "\n",
    "# contents를 스플리터로 자르고 메타데이터 붙이기\n",
    "def split_documents(contents: list[str], metadatas: list[Metadata]):\n",
    "  docs: list[Document] = []\n",
    "\n",
    "  for i in range(len(contents)):\n",
    "    splits = splitter.create_documents(\n",
    "      texts=[contents[i]],\n",
    "      metadatas=[{\n",
    "        \"keyword\":metadatas[i].keyword,\n",
    "        \"link\":metadatas[i].link,\n",
    "      }],\n",
    "    )\n",
    "    docs.extend(splits)\n",
    "  return docs\n",
    "\n",
    "# SearchResult로 가져온 문서들을 contents와 metadatas로 분리\n",
    "def embed_documents(datas: list[SearchResult]):\n",
    "  # 데이터를 content와 메타데이터로 분리\n",
    "  contents: list[str] = [(\"# \" + data.title + \"\\n\" + data.content) for data in datas]\n",
    "  metadatas: list[Metadata] = [\n",
    "    Metadata(\n",
    "      data.keyword,\n",
    "      data.link\n",
    "    ) for data in datas\n",
    "  ]\n",
    "\n",
    "  docs = split_documents(contents=contents, metadatas=metadatas)\n",
    "\n",
    "  # 벡터 스토어에 데이터 저장\n",
    "  vstore.add_documents(docs)\n",
    "  \n",
    "  return {\"result\":\"complete\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb7c29a",
   "metadata": {},
   "source": [
    "### 2. llm 호출\n",
    "#### 1. 프롬프트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a178527c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "해당 내용을 요약하고, 태그와 카테고리를 생성해주십시오.\n",
      "출력 양식은 다음과 같이 Json 형식으로 작성해야합니다: \n",
      "{\n",
      "    \"summarize\"=\"내용 요약\",\n",
      "    \"tags\"=[\n",
      "        \"태그1\",\n",
      "        \"태그2\",\n",
      "        ...\n",
      "    ]\n",
      "    \"category=\"카테고리\"\n",
      "}\n",
      "\n",
      "태그는 아무 단어나 상관 없지만, 카테고리는 다음 중 하나를 선택해야 합니다:\n",
      "- 건강\n",
      "- 게임\n",
      "- 과학\n",
      "- 기술\n",
      "- 기타\n",
      "- 기후\n",
      "- 미용 및 패션\n",
      "- 법률 및 정부\n",
      "- 반려동물 및 동물\n",
      "- 비즈니스 및 금융\n",
      "- 쇼핑\n",
      "- 스포츠\n",
      "- 식음료\n",
      "- 엔터테이먼트\n",
      "- 자동차\n",
      "- 정치\n",
      "- 취업 및 교육\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 분석가\n",
    "analyst_form=\"\"\"\n",
    "{\n",
    "    \"answer\":\"문서를 바탕으로 한 추론과 결론\",\n",
    "    \"link\":[\n",
    "        \"근거로 사용한 문서의 링크 1\",\n",
    "        \"근거로 사용한 문서의 링크 2\",\n",
    "        ....\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def analyst_prompt(keyword: str, form: str):\n",
    "    return f\"\"\"\n",
    "당신은 최고의 검색어 분석 전문가 입니다.\n",
    "입력으로 키워드와 여러 문서들의 내용이 주어집니다.\n",
    "해당 문서들은 모두 {keyword}로 검색된 내용들 중 가장 최근의 기사들입니다. \n",
    "아래와 같은 규칙과 방법으로 현재 이 키워드가 왜 트랜디해졌는지 분석하십시오.\n",
    "\n",
    "근거는 반드시 문서만을 참고하여 주십시오.\n",
    "각 문서마다 핵심 이슈를 요약하여 유사한 문서가 있는지 확인해야합니다.\n",
    "여러개의 문서 중 유사한 주제로 3개 이상 동시에 발견되어야합니다.\n",
    "감정적 단어나 추측적 표현(\"~일 것이다\")은 피하고, 실제 문서 근거를 기반으로 결론을 도출합니다.\\\n",
    "\n",
    "출력 형식은 다음과 같이 JSON으로 주십시오: {form}\n",
    "\"\"\"\n",
    "\n",
    "# 검증자\n",
    "validator_form=\"\"\"\n",
    "{\n",
    "    \"validation\": \"yes\" 또는 \"no\",\n",
    "    \"reason\": \"판단의 이유 한 두 줄로 설명\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def analyst_input_prompt(searched_documents: list[Document], news: list[str]) -> str:\n",
    "    prompt = f\"\"\"\n",
    "다음은 해당 검색어로 검색된 뉴스의 내용입니다. 참고하십시오:\n",
    "{news}\n",
    "\n",
    "이 내용은 해당 검색어와 관련되어있는 것으로 보이는 다른 검색어의\n",
    "뉴스 내용입니다. 필요할 경우 추가적인 추론의 근거로 참고하십시오:\n",
    "\"\"\"\n",
    "    for document in searched_documents:\n",
    "        prompt+= f\"\"\"\n",
    "- {document.page_content}\n",
    "    - keyword: {document.metadata[\"keyword\"]}\n",
    "    - link: {document.metadata[\"link\"]}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def validator_prompt(keyword: str, form: str):\n",
    "    return f\"\"\"\n",
    "당신은 엄격한 리뷰어 입니다.\n",
    "이 내용은 벡터DB에서 {keyword}로 검색되었습니다.\n",
    "실제로 해당 내용과 관련이 있는지 간단한 문구와 함께 검증하시오.\n",
    "\n",
    "다음과 같은 양식으로 한 줄로 작성해야 합니다: {form}\n",
    "\"\"\"\n",
    "\n",
    "# 요약자\n",
    "summarizer_prompt=\"다음 뉴스를 300자 내로 정리하십시오:\"\n",
    "\n",
    "# 분류자\n",
    "# 먼저 정의된 카테고리 목록\n",
    "ALLOWED_CATEGORIES = [\n",
    "    \"건강\", \"게임\", \"과학\", \"기술\", \"기타\", \"기후\",\n",
    "    \"미용 및 패션\", \"법률 및 정부\", \"반려동물 및 동물\",\n",
    "    \"비즈니스 및 금융\", \"쇼핑\", \"스포츠\", \"식음료\",\n",
    "    \"엔터테이먼트\", \"자동차\", \"정치\", \"취업 및 교육\"\n",
    "]\n",
    "\n",
    "classifier_form=\"\"\"\n",
    "{\n",
    "    \"summarize\"=\"내용 요약\",\n",
    "    \"tags\"=[\n",
    "        \"태그1\",\n",
    "        \"태그2\",\n",
    "        ...\n",
    "    ]\n",
    "    \"category=\"카테고리\"\n",
    "}\n",
    "\"\"\"\n",
    "classifier_prompt=f\"\"\"\n",
    "해당 내용을 요약하고, 태그와 카테고리를 생성해주십시오.\n",
    "출력 양식은 다음과 같이 Json 형식으로 작성해야합니다: {classifier_form}\n",
    "태그는 아무 단어나 상관 없지만, 카테고리는 다음 중 하나를 선택해야 합니다:\n",
    "\"\"\"\n",
    "for category in ALLOWED_CATEGORIES:\n",
    "    classifier_prompt += \"- \" + category + \"\\n\"\n",
    "\n",
    "print(classifier_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc6687",
   "metadata": {},
   "source": [
    "#### 2. 호출 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e76cdfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyst_llm(keyword: str, docs: list[Document], summaries: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    분석 llm 호출 함수\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=analyst_prompt(keyword, analyst_form)),\n",
    "        HumanMessage(content=analyst_input_prompt(docs, summaries))\n",
    "    ]\n",
    "\n",
    "    # invoke()는 ChatModel 표준 인터페이스입니다.\n",
    "    result = llm_analize.invoke(messages)\n",
    "\n",
    "    # result는 보통 AIMessage 객체이며 .content에 텍스트가 들어 있습니다.\n",
    "    return result.content.strip()\n",
    "\n",
    "def validator_llm(prompt: str, keyword: str) -> str:\n",
    "    \"\"\"\n",
    "    검증 llm 호출 함수\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=validator_prompt(keyword, validator_form)),\n",
    "        HumanMessage(content=prompt),\n",
    "    ]\n",
    "\n",
    "    # invoke()는 ChatModel 표준 인터페이스입니다.\n",
    "    result = llm_validate.invoke(messages)\n",
    "    \n",
    "    return result.content.strip()\n",
    "\n",
    "def summarizer_llm(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    요약 llm 호출 함수\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=summarizer_prompt),\n",
    "        HumanMessage(content=prompt),\n",
    "    ]\n",
    "\n",
    "    result = llm_summarize.invoke(messages)\n",
    "\n",
    "    # result는 보통 AIMessage 객체이며 .content에 텍스트가 들어 있습니다.\n",
    "    return result.content.strip()\n",
    "\n",
    "def classifier_llm(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    분류 llm 호출 함수\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=classifier_prompt),\n",
    "        HumanMessage(content=prompt),\n",
    "    ]\n",
    "\n",
    "    result = llm_classify.invoke(messages)\n",
    "\n",
    "    # result는 보통 AIMessage 객체이며 .content에 텍스트가 들어 있습니다.\n",
    "    return result.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24fee9",
   "metadata": {},
   "source": [
    "### 3. 크롤링 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d3e783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "def get_keyword_news(keyword):\n",
    "    href_links = []\n",
    "\n",
    "    origin_url = f'https://search.naver.com/search.naver?ssc=tab.news.all&query={keyword}'\n",
    "    origin_url += \"&sm=tab_opt&sort=0&photo=0&field=0&pd=12&ds=&de=&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Aall&is_sug_officeid=0&office_category=0&service_area=0\"\n",
    "\n",
    "    response = requests.get(origin_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    naver_spans = soup.find_all('span', string='네이버뉴스')\n",
    "   \n",
    "    for news in naver_spans:\n",
    "        anchor_tag = news.find('a')\n",
    "        if anchor_tag:\n",
    "            href = anchor_tag.get('href')\n",
    "            href_links.append(href)\n",
    "    return href_links\n",
    "    \n",
    "def get_news_from_naver(keyword, urls) -> list[SearchResult]:\n",
    "    results: list[SearchResult] = []\n",
    "    title = \"\"\n",
    "    content = \"\"\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        if 'entertain.naver.com' in url:\n",
    "            title_tag = soup.find('div', class_='ArticleHead_article_head_title__YUNFf')\n",
    "            content_tag = soup.find('div', class_='_article_content')\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            content = content_tag.get_text(strip=True)\n",
    "        \n",
    "        elif 'n.news.naver.com' in url:\n",
    "            title_tag = soup.find('div', class_='media_end_head_title')\n",
    "            content_tag = soup.find('div', class_='newsct_article')\n",
    "            title = title_tag.find('span').get_text(strip=True)\n",
    "            content = content_tag.get_text(strip=True)\n",
    "        elif 'm.sports.naver.com' in url:\n",
    "            title_tag = soup.find('h2', class_='ArticleHead_article_title__qh8GV')\n",
    "            content_tag = soup.find('div', class_='ArticleContent_comp_article_content__luOFM')\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            content = content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            print(\"Unsupported URL format:\", url)\n",
    "\n",
    "        results.append(SearchResult(\n",
    "            keyword=keyword,\n",
    "            link=url,\n",
    "            title=title,\n",
    "            content=content\n",
    "        ))\n",
    "        # time.sleep(0.2)\n",
    "    return results\n",
    "\n",
    "# 진짜 쓰는 놈\n",
    "def news_crawling(keyword):\n",
    "    href_links = get_keyword_news(keyword)\n",
    "    news_results = get_news_from_naver(keyword, href_links)\n",
    "    return news_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479cc178",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. 그래프 요소 정의\n",
    "### 1. 상태 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb59164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "State = {\n",
    "    \"keyword\": str,                # 분석 대상 키워드\n",
    "    \"raw_documents\": [SearchResult],   # 크롤링/뉴스 등 원문 문서들 (content, link, source 등)\n",
    "    \"embedded_documents\": [dict],  # 임베딩 후 VDB에 저장된 문서 메타\n",
    "    \"retrieved_documents\": [Document], # VDB에서 keyword 기반으로 가져온 후보 문서들\n",
    "    \"validated_documents\": [Document], # 키워드와 실제로 관련 있다고 LLM이 yes 준 문서들\n",
    "    \"news_summaries\": [str],       # 뉴스 개별 요약 결과들\n",
    "    \"trend_analysis\": dict,        # 트렌드 원인 분석 (LLM 결과: answer + link[])\n",
    "    \"summarize_and_classify\": dict           # 최종 JSON (keyword, description, content, tags, category, refered)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f2e674",
   "metadata": {},
   "source": [
    "### 2. 노드 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e6bc7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 수집 부분\n",
    "def collect_sources(state):\n",
    "    print(f\"데이터 수집 중: {state[\"keyword\"]}\")\n",
    "    # 1) keyword로 외부 소스 크롤링\n",
    "    fetched_docs = news_crawling(state[\"keyword\"])\n",
    "    # 2) 결과를 [{\"keyword\": ..., \"link\": ..., \"content\": ...}, ...] 형태로 수집\n",
    "    state[\"raw_documents\"] = fetched_docs\n",
    "    return state\n",
    "\n",
    "# 데이터 임베딩\n",
    "def embed_and_store(state):\n",
    "    print(\"수집한 문서 임베딩 중\")\n",
    "    # 문서별 embedding 계산 -> VDB에 저장\n",
    "    docs = embed_documents(state[\"raw_documents\"])\n",
    "    # 저장 후, vector_id 등 메타 정리\n",
    "    # 저장 결과 출력\n",
    "    state[\"embedded_documents\"] = docs\n",
    "    return state\n",
    "\n",
    "# VDB에서 Retrieve\n",
    "def retrieve_from_vdb(state):\n",
    "    print(\"데이터 검색 중\")\n",
    "    retrieved = search_documents(state[\"keyword\"])\n",
    "    state[\"retrieved_documents\"] = retrieved  # 예: [{\"content\":..., \"link\":..., ...}, ...]\n",
    "    return state\n",
    "\n",
    "# 각 문서 검증\n",
    "def validate_relevance(state):\n",
    "    print(\"각 문서 검증\")\n",
    "    validated = []\n",
    "    if len(state[\"retrieved_documents\"]) == 0:\n",
    "        return state\n",
    "    for doc in state[\"retrieved_documents\"]:\n",
    "        raw = validator_llm(keyword=state[\"keyword\"], prompt=doc.page_content)\n",
    "        judgment = json.loads(raw)\n",
    "        if judgment[\"validation\"] == \"yes\":\n",
    "            validated.append(Document(\n",
    "                page_content= doc.page_content,\n",
    "                metadata= {**doc.metadata, \"reason\": judgment[\"reason\"]},\n",
    "            ))\n",
    "    state[\"validated_documents\"] = validated\n",
    "    return state\n",
    "\n",
    "# 원문 요약\n",
    "def summarize_news_individual(state):\n",
    "    print(\"원문 요약 중\")\n",
    "    summaries = []\n",
    "    for doc in state[\"raw_documents\"]:\n",
    "        summary = summarizer_llm(doc.content)\n",
    "        summaries.append({\n",
    "            \"link\": doc.link,\n",
    "            \"summary\": summary\n",
    "        })\n",
    "    state[\"news_summaries\"] = summaries\n",
    "    return state\n",
    "\n",
    "# 분석 및 이유 작성\n",
    "def analyze_trend_reason(state):\n",
    "    print(\"최종 분석 중\")\n",
    "    raw = analyst_llm(\n",
    "        keyword=state[\"keyword\"],\n",
    "        docs=state[\"validated_documents\"],\n",
    "        summaries=state[\"news_summaries\"]\n",
    "    )\n",
    "    trend_json = json.loads(raw)\n",
    "    state[\"trend_analysis\"] = trend_json  # {\"answer\": \"...\", \"link\": [\"...\", \"...\"]}\n",
    "    return state\n",
    "\n",
    "# 태그, 카테고리 붙이기\n",
    "def classify_and_package(state):\n",
    "    print(\"태그, 카테고리 붙이는 중 \")\n",
    "    packaged = classifier_llm(\n",
    "        prompt=state[\"trend_analysis\"][\"answer\"],\n",
    "    )\n",
    "    state[\"summarize_and_classify\"] = packaged\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faaa954",
   "metadata": {},
   "source": [
    "### 3. 노드 잇기 (그래프 짜기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b61ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "workflow = StateGraph(dict)  # dict 대신 위에서 정의한 State 모델 사용 권장\n",
    "\n",
    "workflow.add_node(\"collect_sources\", collect_sources)\n",
    "workflow.add_node(\"embed_and_store\", embed_and_store)\n",
    "workflow.add_node(\"retrieve_from_vdb\", retrieve_from_vdb)\n",
    "workflow.add_node(\"validate_relevance\", validate_relevance)\n",
    "workflow.add_node(\"summarize_news_individual\", summarize_news_individual)\n",
    "workflow.add_node(\"analyze_trend_reason\", analyze_trend_reason)\n",
    "workflow.add_node(\"classify_and_package\", classify_and_package)\n",
    "\n",
    "workflow.add_edge(\"collect_sources\", \"embed_and_store\")\n",
    "workflow.add_edge(\"embed_and_store\", \"retrieve_from_vdb\")\n",
    "workflow.add_edge(\"retrieve_from_vdb\", \"validate_relevance\")\n",
    "workflow.add_edge(\"validate_relevance\", \"summarize_news_individual\")\n",
    "workflow.add_edge(\"summarize_news_individual\", \"analyze_trend_reason\")\n",
    "workflow.add_edge(\"analyze_trend_reason\", \"classify_and_package\")\n",
    "\n",
    "workflow.set_entry_point(\"collect_sources\")\n",
    "workflow.set_finish_point(\"classify_and_package\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "990f184b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 수집 중: 호남대\n",
      "수집한 문서 임베딩 중\n",
      "데이터 검색 중\n",
      "각 문서 검증\n",
      "원문 요약 중\n",
      "최종 분석 중\n",
      "태그, 카테고리 붙이는 중 \n"
     ]
    }
   ],
   "source": [
    "initial_state = {\"keyword\":\"호남대\"}\n",
    "output = app.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dc77668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ YAML 저장 완료\n"
     ]
    }
   ],
   "source": [
    "# 검색결과 저장하기\n",
    "## 보기 쉽게..\n",
    "from dataclasses import is_dataclass, asdict\n",
    "import yaml\n",
    "def to_plain(obj):\n",
    "    # dataclass → dict\n",
    "    if is_dataclass(obj):\n",
    "        return asdict(obj)\n",
    "    # LangChain Document → dict\n",
    "    if isinstance(obj, Document):\n",
    "        return {\n",
    "            \"page_content\": obj.page_content,\n",
    "            \"metadata\": obj.metadata,\n",
    "        }\n",
    "    # dict → 재귀 변환\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: to_plain(v) for k, v in obj.items()}\n",
    "    # list, tuple, set → 재귀 변환\n",
    "    if isinstance(obj, (list, tuple, set)):\n",
    "        return [to_plain(v) for v in obj]\n",
    "    # 기본 타입은 그대로\n",
    "    return obj\n",
    "\n",
    "# 변환 후 YAML로 저장\n",
    "with open(\"output.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.safe_dump(to_plain(output), f, allow_unicode=True, sort_keys=False)\n",
    "\n",
    "print(\"✅ YAML 저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fd509f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summarize': \"호남대학교는 최근 농협광주본부와 함께 '아침밥먹기 캠페인'을 개최하여 학생들의 건강과 활력을 높이는 데 힘쓰고 있습니다. 또한, 광주 우호협력도시인 중국 뤄양시의 의료미용 연수단이 호남대학교를 방문하여 미용·뷰티 전문 교육과정을 체험하고 수료증을 취득하는 등 교류·협력 방안을 모색하고 있습니다. 광주시자원봉사센터의 자원봉사 시민의식 조사에서 시민들이 관심을 갖고 있는 자원봉사 분야 중 하나로 호남대학교가 언급되고 있습니다.\",\n",
       " 'tags': ['호남대학교',\n",
       "  '농협광주본부',\n",
       "  '아침밥먹기 캠페인',\n",
       "  '건강',\n",
       "  '교류',\n",
       "  '협력',\n",
       "  '의료미용',\n",
       "  '미용뷰티',\n",
       "  '자원봉사',\n",
       "  '시민의식 조사'],\n",
       " 'category': '기타'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_summarize = json.loads(output[\"summarize_and_classify\"])\n",
    "get_summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d4ce746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SearchResultOutput(keyword='호남대', description=\"호남대학교는 최근 농협광주본부와 함께 '아침밥먹기 캠페인'을 개최하여 학생들의 건강과 활력을 높이는 데 힘쓰고 있습니다. 또한, 광주 우호협력도시인 중국 뤄양시의 의료미용 연수단이 호남대학교를 방문하여 미용·뷰티 전문 교육과정을 체험하고 수료증을 취득하는 등 교류·협력 방안을 모색하고 있습니다. 광주시자원봉사센터의 자원봉사 시민의식 조사에서 시민들이 관심을 갖고 있는 자원봉사 분야 중 하나로 호남대학교가 언급되고 있습니다.\", content=\"호남대학교는 최근 다양한 분야에서 활동을 펼치고 있습니다. 농협광주본부와 함께 '아침밥먹기 캠페인'을 개최하여 학생들의 건강과 활력을 높이는 데 힘쓰고 있습니다. 또한, 광주 우호협력도시인 중국 뤄양시의 의료미용 연수단이 호남대학교를 방문하여 미용·뷰티 전문 교육과정을 체험하고 수료증을 취득하는 등 교류·협력 방안을 모색하고 있습니다. 이 외에도 광주시자원봉사센터의 자원봉사 시민의식 조사에서 시민들이 관심을 갖고 있는 자원봉사 분야 중 하나로 호남대학교가 언급되고 있습니다. 따라서 호남대학교는 최근 다양한 분야에서의 활동과 협력으로 인해 검색어 트렌드에 오른 것으로 추론됩니다.\", tags=['호남대학교', '농협광주본부', '아침밥먹기 캠페인', '건강', '교류', '협력', '의료미용', '미용뷰티', '자원봉사', '시민의식 조사'], category='기타', refered=['https://n.news.naver.com/mnews/article/003/0013569803?sid=102', 'https://n.news.naver.com/mnews/article/016/0002549842?sid=102', 'https://m.sports.naver.com/general/article/449/0000324932', 'https://n.news.naver.com/mnews/article/030/0003364926?sid=102', 'https://n.news.naver.com/mnews/article/003/0013569974?sid=102', 'https://n.news.naver.com/mnews/article/003/0013568405?sid=102', 'https://m.sports.naver.com/general/article/382/0001232719'])\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class SearchResultOutput:\n",
    "    \"\"\"\n",
    "    검색에 대한 응답 클래스입니다.\n",
    "    \"\"\"\n",
    "\n",
    "    keyword: str\n",
    "    description: str\n",
    "    content: str\n",
    "    tags: list[str]\n",
    "    category: str\n",
    "    refered: list[str]\n",
    "\n",
    "output_real = SearchResultOutput(\n",
    "    keyword=output[\"keyword\"],\n",
    "    description=get_summarize['summarize'],\n",
    "    content=output[\"trend_analysis\"][\"answer\"],\n",
    "    tags=get_summarize['tags'],\n",
    "    category=get_summarize['category'],\n",
    "    refered=output[\"trend_analysis\"][\"link\"]\n",
    ")\n",
    "\n",
    "print(output_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e3ba60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62599a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
