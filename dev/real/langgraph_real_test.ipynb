{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2fadd8",
   "metadata": {},
   "source": [
    "# 실제 로직 테스트 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbddf68c",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c09d2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환경 변수 로드\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ad86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 클래스 목록\n",
    "from dataclasses import dataclass\n",
    "\n",
    "## 검색될 내용\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "  keyword: str\n",
    "  title: str\n",
    "  link: str\n",
    "  content: str\n",
    "\n",
    "## VDB에 들어갈 메타 데이터\n",
    "@dataclass\n",
    "class Metadata:\n",
    "  keyword: str\n",
    "  link: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb06abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "# 답변 모델\n",
    "llm_main = ChatUpstage(\n",
    "    model=\"solar-mini\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# 검증 모델\n",
    "llm_validate = ChatUpstage(\n",
    "    model=\"solar-pro2\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c5de9",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 사전 요소 정의 \n",
    "### 1. RAG 기능 쪽 변수 및 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "241f3e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# 스플리터 정의\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=300,\n",
    "  chunk_overlap=50\n",
    ")\n",
    "\n",
    "# 임베딩 모델 설정\n",
    "vstore = Chroma(\n",
    "  collection_name=\"knowledge_base\",\n",
    "  embedding_function=UpstageEmbeddings(model=\"embedding-query\"),\n",
    "  persist_directory=\"./chroma\"\n",
    ")\n",
    "\n",
    "# 검색자\n",
    "retriever=vstore.as_retriever(\n",
    "  search_type=\"mmr\",\n",
    "  search_kwargs={\n",
    "    \"k\":4,\n",
    "    \"fetch_k\":20,\n",
    "    \"lambda_mult\":0.5\n",
    "  }\n",
    ")\n",
    "\n",
    "# 연관 되어있는 것으로 볼 범위\n",
    "THRESHOLD = 1.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca8a51bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# VDB에 저장된 문서 묶음 검색해오기\n",
    "def search_documents(keyword: str) -> list[Document]:\n",
    "  scored = vstore.similarity_search_with_score(keyword, k=20)\n",
    "\n",
    "  filtered =  [\n",
    "    doc for doc, score in scored if score <= THRESHOLD\n",
    "  ]\n",
    "  return filtered\n",
    "\n",
    "# 검색 후 프롬프트도 만들기\n",
    "def get_prompt_on_retrieve(keyword: str, prompt: str) -> str:\n",
    "  search_results = search_documents(keyword=keyword)\n",
    "\n",
    "  # 프롬프트에 내용 삽입하기\n",
    "  result = prompt\n",
    "  for items in search_results:\n",
    "     result += (\"\\n\" + items.page_content)\n",
    "\n",
    "  return result\n",
    "\n",
    "# contents를 스플리터로 자르고 메타데이터 붙이기\n",
    "def split_documents(contents: list[str], metadatas: list[Metadata]):\n",
    "  docs: list[Document] = []\n",
    "\n",
    "  for i in range(len(contents)):\n",
    "    splits = splitter.create_documents(\n",
    "      texts=[contents[i]],\n",
    "      metadatas=[{\n",
    "        \"keyword\":metadatas[i].keyword,\n",
    "        \"link\":metadatas[i].link,\n",
    "      }],\n",
    "    )\n",
    "    docs.extend(splits)\n",
    "  return docs\n",
    "\n",
    "# SearchResult로 가져온 문서들을 contents와 metadatas로 분리\n",
    "def embed_documents(datas: list[SearchResult]):\n",
    "  # 데이터를 content와 메타데이터로 분리\n",
    "  contents: list[str] = [(\"# \" + data.title + \"\\n\" + data.content) for data in datas]\n",
    "  metadatas: list[Metadata] = [\n",
    "    Metadata(\n",
    "      data.keyword,\n",
    "      data.link\n",
    "    ) for data in datas\n",
    "  ]\n",
    "\n",
    "  docs = split_documents(contents=contents, metadatas=metadatas)\n",
    "\n",
    "  # 벡터 스토어에 데이터 저장\n",
    "  vstore.add_documents(docs)\n",
    "  \n",
    "  return {\"result\":\"complete\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb7c29a",
   "metadata": {},
   "source": [
    "### 2. llm 모델 호출 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a178527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_form=\"\"\"\n",
    "{\n",
    "    \"answer\":\"문서를 바탕으로 한 추론과 결론\",\n",
    "    \"link\":[\n",
    "        \"근거로 사용한 문서의 링크 1\",\n",
    "        \"근거로 사용한 문서의 링크 2\",\n",
    "        ....\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def analyst_prompt(keyword: str, form: str):\n",
    "    return f\"\"\"\n",
    "당신은 최고의 검색어 분석 전문가 입니다.\n",
    "입력으로 키워드와 여러 문서들의 내용이 주어집니다.\n",
    "해당 문서들은 모두 {keyword}로 검색된 내용들 중 가장 최근의 기사들입니다. \n",
    "아래와 같은 규칙과 방법으로 현재 이 키워드가 왜 트랜디해졌는지 분석하십시오.\n",
    "\n",
    "근거는 반드시 문서만을 참고하여 주십시오.\n",
    "각 문서마다 핵심 이슈를 요약하여 유사한 문서가 있는지 확인해야합니다.\n",
    "여러개의 문서 중 유사한 주제로 3개 이상 동시에 발견되어야합니다.\n",
    "감정적 단어나 추측적 표현(\"~일 것이다\")은 피하고, 실제 문서 근거를 기반으로 결론을 도출합니다.\\\n",
    "\n",
    "출력 형식은 다음과 같이 JSON으로 주십시오: {form}\n",
    "\"\"\"\n",
    "\n",
    "validator_form=\"\"\"\n",
    "{\n",
    "    \"validation\": \"yes\" 또는 \"no\",\n",
    "    \"reason\": \"판단의 이유 한 두 줄로 설명\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def validator_prompt(keyword: str, form: str):\n",
    "    return f\"\"\"\n",
    "당신은 엄격한 리뷰어 입니다.\n",
    "이 내용은 벡터DB에서 {keyword}로 검색되었습니다.\n",
    "실제로 해당 내용과 관련이 있는지 간단한 문구와 함께 검증하시오.\n",
    "\n",
    "다음과 같은 양식으로 한 줄로 작성해야 합니다: {form}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e76cdfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyst_llm(prompt: str, keyword: str) -> str:\n",
    "    \"\"\"\n",
    "    메인 llm 호출 함수\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=analyst_prompt(keyword, analyst_form)),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "\n",
    "    # invoke()는 ChatModel 표준 인터페이스입니다.\n",
    "    result = llm_main.invoke(messages)\n",
    "\n",
    "    # result는 보통 AIMessage 객체이며 .content에 텍스트가 들어 있습니다.\n",
    "    return result.content.strip()\n",
    "\n",
    "def validator_llm(prompt: str, keyword: str) -> str:\n",
    "    \"\"\"\n",
    "    검증 llm 호출 함수\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=validator_prompt(keyword, validator_form)),\n",
    "        HumanMessage(content=prompt),\n",
    "    ]\n",
    "\n",
    "    # invoke()는 ChatModel 표준 인터페이스입니다.\n",
    "    result = llm_validate.invoke(messages)\n",
    "\n",
    "    # result는 보통 AIMessage 객체이며 .content에 텍스트가 들어 있습니다.\n",
    "    return result.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24fee9",
   "metadata": {},
   "source": [
    "### 3. 크롤링 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d3e783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "def get_keyword_news(keyword):\n",
    "    href_links = []\n",
    "\n",
    "    origin_url = f'https://search.naver.com/search.naver?where=news&query={keyword}'\n",
    "\n",
    "    response = requests.get(origin_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    naver_spans = soup.find_all('span', string='네이버뉴스')\n",
    "   \n",
    "    for news in naver_spans:\n",
    "        anchor_tag = news.find('a')\n",
    "        if anchor_tag:\n",
    "            href = anchor_tag.get('href')\n",
    "            href_links.append(href)\n",
    "    return href_links\n",
    "    \n",
    "def get_news_from_naver(keyword, urls) -> list[SearchResult]:\n",
    "    results: list[SearchResult] = []\n",
    "    title = \"\"\n",
    "    content = \"\"\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        if 'entertain.naver.com' in url:\n",
    "            title_tag = soup.find('div', class_='ArticleHead_article_head_title__YUNFf')\n",
    "            content_tag = soup.find('div', class_='_article_content')\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            content = content_tag.get_text(strip=True)\n",
    "        \n",
    "        elif 'n.news.naver.com' in url:\n",
    "            title_tag = soup.find('div', class_='media_end_head_title')\n",
    "            content_tag = soup.find('div', class_='newsct_article')\n",
    "            title = title_tag.find('span').get_text(strip=True)\n",
    "            content = content_tag.get_text(strip=True)\n",
    "        elif 'm.sports.naver.com' in url:\n",
    "            title_tag = soup.find('h2', class_='ArticleHead_article_title__qh8GV')\n",
    "            content_tag = soup.find('div', class_='ArticleContent_comp_article_content__luOFM')\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            content = content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            print(\"Unsupported URL format:\", url)\n",
    "\n",
    "        results.append(SearchResult(\n",
    "            keyword=keyword,\n",
    "            link=url,\n",
    "            title=title,\n",
    "            content=content\n",
    "        ))\n",
    "        # time.sleep(0.2)\n",
    "    return results\n",
    "\n",
    "# 진짜 쓰는 놈\n",
    "def news_crawling(keyword):\n",
    "    href_links = get_keyword_news(keyword)\n",
    "    news_results = get_news_from_naver(keyword, href_links)\n",
    "    return news_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479cc178",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. 그래프 요소 정의\n",
    "### 1. 상태 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "State = {\n",
    "    \"keyword\": str,                # 분석 대상 키워드\n",
    "    \"raw_documents\": [SearchResult],   # 크롤링/뉴스 등 원문 문서들 (content, link, source 등)\n",
    "    \"embedded_documents\": [dict],  # 임베딩 후 VDB에 저장된 문서 메타\n",
    "    \"retrieved_documents\": [dict], # VDB에서 keyword 기반으로 가져온 후보 문서들\n",
    "    \"validated_documents\": [dict], # 키워드와 실제로 관련 있다고 LLM이 yes 준 문서들\n",
    "    \"news_summaries\": [str],       # 뉴스 개별 요약 결과들\n",
    "    \"trend_analysis\": dict,        # 트렌드 원인 분석 (LLM 결과: answer + link[])\n",
    "    \"final_result\": dict           # 최종 JSON (keyword, description, content, tags, category, refered)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f2e674",
   "metadata": {},
   "source": [
    "### 2. 노드 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6bc7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 수집 부분\n",
    "def collect_sources(state):\n",
    "    print(f\"데이터 수집 중: {state[\"keyword\"]}\")\n",
    "    # 1) keyword로 외부 소스 크롤링\n",
    "    fetched_docs = news_crawling(state[\"keyword\"])\n",
    "    # 2) 결과를 [{\"keyword\": ..., \"link\": ..., \"content\": ...}, ...] 형태로 수집\n",
    "    state[\"raw_documents\"] = fetched_docs\n",
    "    return state\n",
    "\n",
    "# 데이터 임베딩\n",
    "def embed_and_store(state):\n",
    "    print(\"수집한 문서 임베딩 중\")\n",
    "    # 문서별 embedding 계산 -> VDB에 저장\n",
    "    docs = embed_documents(state[\"raw_documents\"])\n",
    "    # 저장 후, vector_id 등 메타 정리\n",
    "    # 저장 결과 출력\n",
    "    state[\"embedded_documents\"] = docs\n",
    "    return state\n",
    "\n",
    "# VDB에서 Retrieve\n",
    "def retrieve_from_vdb(state):\n",
    "    print(\"데이터 검색 중\")\n",
    "    retrieved = search_documents(state[\"keyword\"])\n",
    "    state[\"retrieved_documents\"] = retrieved  # 예: [{\"content\":..., \"link\":..., ...}, ...]\n",
    "    return state\n",
    "\n",
    "# 각 문서 검증\n",
    "def validate_relevance(state):\n",
    "    print(\"각 문서 검증\")\n",
    "    validated = []\n",
    "    for doc in state[\"retrieved_documents\"]:\n",
    "        judgment = llm_validate(keyword=state[\"keyword\"], doc=doc[\"content\"])\n",
    "        if judgment[\"validation\"] == \"yes\":\n",
    "            validated.append({**doc, \"reason\": judgment[\"reason\"]})\n",
    "    state[\"validated_documents\"] = validated\n",
    "    return state\n",
    "\n",
    "# 원문 요약\n",
    "def summarize_news_individual(state):\n",
    "    print(\"원문 요약 중\")\n",
    "    summaries = []\n",
    "    for doc in state[\"raw_documents\"]:\n",
    "        summary = llm_main(doc[\"content\"])\n",
    "        summaries.append({\n",
    "            \"link\": doc[\"link\"],\n",
    "            \"summary\": summary\n",
    "        })\n",
    "    state[\"news_summaries\"] = summaries\n",
    "    return state\n",
    "\n",
    "# 분석 및 이유 작성\n",
    "def analyze_trend_reason(state):\n",
    "    print(\"최종 분석 중\")\n",
    "    trend_json = llm_main(\n",
    "        keyword=state[\"keyword\"],\n",
    "        docs=state[\"validated_documents\"],\n",
    "        summaries=state[\"news_summaries\"]\n",
    "    )\n",
    "    state[\"trend_analysis\"] = trend_json  # {\"answer\": \"...\", \"link\": [\"...\", \"...\"]}\n",
    "    return state\n",
    "\n",
    "# 먼저 정의된 카테고리 목록\n",
    "ALLOWED_CATEGORIES = [\n",
    "    \"건강\", \"게임\", \"과학\", \"기술\", \"기타\", \"기후\",\n",
    "    \"미용 및 패션\", \"법률 및 정부\", \"반려동물 및 동물\",\n",
    "    \"비즈니스 및 금융\", \"쇼핑\", \"스포츠\", \"식음료\",\n",
    "    \"엔터테이먼트\", \"자동차\", \"정치\", \"취업 및 교육\"\n",
    "]\n",
    "\n",
    "# 태그, 카테고리 붙이기\n",
    "def classify_and_package(state):\n",
    "    print(\"태그, 카테고리 붙이는 중 \")\n",
    "    packaged = llm_main(\n",
    "        keyword=state[\"keyword\"],\n",
    "        trend_answer=state[\"trend_analysis\"][\"answer\"],\n",
    "        links=state[\"trend_analysis\"][\"link\"],\n",
    "        allowed_categories=ALLOWED_CATEGORIES,\n",
    "    )\n",
    "    state[\"final_result\"] = packaged\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faaa954",
   "metadata": {},
   "source": [
    "### 3. 노드 잇기 (그래프 짜기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b61ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "workflow = StateGraph(dict)  # dict 대신 위에서 정의한 State 모델 사용 권장\n",
    "\n",
    "workflow.add_node(\"collect_sources\", collect_sources)\n",
    "workflow.add_node(\"embed_and_store\", embed_and_store)\n",
    "workflow.add_node(\"retrieve_from_vdb\", retrieve_from_vdb)\n",
    "workflow.add_node(\"validate_relevance\", validate_relevance)\n",
    "workflow.add_node(\"summarize_news_individual\", summarize_news_individual)\n",
    "workflow.add_node(\"analyze_trend_reason\", analyze_trend_reason)\n",
    "workflow.add_node(\"classify_and_package\", classify_and_package)\n",
    "\n",
    "workflow.add_edge(\"collect_sources\", \"embed_and_store\")\n",
    "workflow.add_edge(\"embed_and_store\", \"retrieve_from_vdb\")\n",
    "workflow.add_edge(\"retrieve_from_vdb\", \"validate_relevance\")\n",
    "workflow.add_edge(\"validate_relevance\", \"summarize_news_individual\")\n",
    "workflow.add_edge(\"summarize_news_individual\", \"analyze_trend_reason\")\n",
    "workflow.add_edge(\"analyze_trend_reason\", \"classify_and_package\")\n",
    "\n",
    "workflow.set_entry_point(\"collect_sources\")\n",
    "workflow.set_finish_point(\"classify_and_package\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "990f184b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 수집 중: 금융투자협회\n",
      "수집한 문서 임베딩 중\n",
      "데이터 검색 중\n",
      "각 문서 검증\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Document' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m금융투자협회\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 2\u001b[0m app\u001b[38;5;241m.\u001b[39minvoke(initial_state)\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2025.06-0/lib/python3.13/site-packages/langgraph/pregel/main.py:3094\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3091\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3092\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 3094\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   3095\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3096\u001b[0m     config,\n\u001b[1;32m   3097\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m   3098\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[1;32m   3101\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[1;32m   3102\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   3103\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   3104\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   3105\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[1;32m   3106\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3107\u001b[0m ):\n\u001b[1;32m   3108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3109\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2025.06-0/lib/python3.13/site-packages/langgraph/pregel/main.py:2679\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2678\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2679\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2680\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2681\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2682\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2683\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2684\u001b[0m ):\n\u001b[1;32m   2685\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2686\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2687\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2688\u001b[0m     )\n\u001b[1;32m   2689\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2025.06-0/lib/python3.13/site-packages/langgraph/pregel/_runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     run_with_retry(\n\u001b[1;32m    168\u001b[0m         t,\n\u001b[1;32m    169\u001b[0m         retry_policy,\n\u001b[1;32m    170\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    171\u001b[0m             CONFIG_KEY_CALL: partial(\n\u001b[1;32m    172\u001b[0m                 _call,\n\u001b[1;32m    173\u001b[0m                 weakref\u001b[38;5;241m.\u001b[39mref(t),\n\u001b[1;32m    174\u001b[0m                 retry_policy\u001b[38;5;241m=\u001b[39mretry_policy,\n\u001b[1;32m    175\u001b[0m                 futures\u001b[38;5;241m=\u001b[39mweakref\u001b[38;5;241m.\u001b[39mref(futures),\n\u001b[1;32m    176\u001b[0m                 schedule_task\u001b[38;5;241m=\u001b[39mschedule_task,\n\u001b[1;32m    177\u001b[0m                 submit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[1;32m    178\u001b[0m             ),\n\u001b[1;32m    179\u001b[0m         },\n\u001b[1;32m    180\u001b[0m     )\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2025.06-0/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2025.06-0/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:656\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 656\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2025.06-0/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:400\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 400\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[10], line 32\u001b[0m, in \u001b[0;36mvalidate_relevance\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     30\u001b[0m validated \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieved_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 32\u001b[0m     judgment \u001b[38;5;241m=\u001b[39m llm_validate(keyword\u001b[38;5;241m=\u001b[39mstate[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword\u001b[39m\u001b[38;5;124m\"\u001b[39m], doc\u001b[38;5;241m=\u001b[39mdoc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m judgment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     34\u001b[0m         validated\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdoc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreason\u001b[39m\u001b[38;5;124m\"\u001b[39m: judgment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreason\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Document' object is not subscriptable",
      "\u001b[0mDuring task with name 'validate_relevance' and id 'e497a8a0-239f-2299-8b89-0146192ef956'"
     ]
    }
   ],
   "source": [
    "initial_state = {\"keyword\":\"금융투자협회\"}\n",
    "app.invoke(initial_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
