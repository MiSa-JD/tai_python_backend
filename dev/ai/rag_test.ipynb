{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5588b3ca",
   "metadata": {},
   "source": [
    "# RAG 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3268067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --disable-pip-version-check -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "982791df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in /Users/jeong-ug/.pyenv/versions/anaconda3-2025.06-0/lib/python3.13/site-packages (from pdfplumber) (11.1.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/jeong-ug/.pyenv/versions/anaconda3-2025.06-0/lib/python3.13/site-packages (from pdfminer.six==20250506->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/jeong-ug/.pyenv/versions/anaconda3-2025.06-0/lib/python3.13/site-packages (from pdfminer.six==20250506->pdfplumber) (44.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/jeong-ug/.pyenv/versions/anaconda3-2025.06-0/lib/python3.13/site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /Users/jeong-ug/.pyenv/versions/anaconda3-2025.06-0/lib/python3.13/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.21)\n",
      "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
      "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pdfplumber]━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pdfplumber]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ec56a",
   "metadata": {},
   "source": [
    "## 1. 환경 변수 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce85c762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환경 변수 로드\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0a2be7",
   "metadata": {},
   "source": [
    "## 2. 문서 스캔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5134acf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./docs/testmd.md', './docs/testtest.md', './docs/fakemd.txt', './docs/pdf/AI Trend with LLM.pdf', './docs/md/testmdmd.md']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 문서가 있는 디렉토리 지정\n",
    "directory = \"./docs\"\n",
    "\n",
    "all_files = []\n",
    "docs = []\n",
    "\n",
    "for root, dirs, files in os.walk(directory):\n",
    "  for file in files:\n",
    "    full_path = os.path.join(root, file)\n",
    "    all_files.append(full_path)\n",
    "\n",
    "print(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7806bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 확장자 감지하기\n",
    "def checkExtension(extention: str, files: str):\n",
    "  \"\"\"\n",
    "  extention: 확장자명 (md, pdf 등) <br/>\n",
    "  files: 파일 목록 <br/>\n",
    "  -> str[]\n",
    "  \"\"\"\n",
    "  results = []\n",
    "  for file in files:\n",
    "    file_name = file.split('.')\n",
    "    if (file_name[-1] == extention):\n",
    "      results.append(file)\n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0db825a",
   "metadata": {},
   "source": [
    "### 사용할 문서가 마크다운일 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91647372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감지된 markdown 자료: ['./docs/testmd.md', './docs/testtest.md', './docs/md/testmdmd.md']\n",
      "파일 읽음: ./docs/testmd.md\n",
      "파일 읽음: ./docs/testtest.md\n",
      "파일 읽음: ./docs/md/testmdmd.md\n",
      "# testmd.md\n",
      "asdfa\n",
      "asdfadfadsfa\n",
      "asdfasfdfasfdsfass\n",
      "\n",
      "# ㅁㄴㅇㄹㅁㄴㅇㄹㄴㄹ\n",
      "\n",
      "# testtest.md\n",
      "asdfasdsfsfsafdsf\n",
      "\n",
      "# testmdmd.md\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_md_files(files):\n",
    "  \"\"\"\n",
    "  files: 배열로 변환할 파일 목록 <br/>\n",
    "  -> str[]\n",
    "  \"\"\"\n",
    "  md_files = checkExtension('md', files)\n",
    "\n",
    "  print(\"감지된 markdown 자료: \", end=\"\")\n",
    "  print(md_files)\n",
    "\n",
    "  # 실제로 파일 읽어서 배열에 집어넣기\n",
    "  md_docs = []\n",
    "\n",
    "  for file in md_files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "      print(\"파일 읽음: \"+file)\n",
    "      text = f.read()\n",
    "      text = text.lower()\n",
    "      title = file.split('/')[-1]\n",
    "      text = f\"# {title}\\n\" + text\n",
    "      md_docs.append(text)\n",
    "  return md_docs\n",
    "\n",
    "for a in get_md_files(all_files):\n",
    "  print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e827b6c",
   "metadata": {},
   "source": [
    "### 사용할 문서가 PDF 일 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae8294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감지된 pdf 자료: ['./docs/pdf/AI Trend with LLM.pdf']\n",
      "LLM Trend (feat. Llama)\n",
      "LLM, SLM, RAG, Agent, ... ?\n",
      "뭐가 너무 많은데\n",
      "김용담 강사\n",
      "\b\n",
      "Contents\n",
      "1. Llama 3.1 LLM Trend\n",
      "을 중심으로 살펴보는\n",
      "2. Llama 3.2 Large Multi-Model\n",
      "를 통해 보는 의 이해\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "4. LLM App\n",
      "구축시 고려할 사항들에 대한 이해\n",
      "\b\n",
      "1. Llama 3.1 LLM Trend\n",
      "을 중심으로 살펴보는\n",
      "\b\n",
      "Language Model ?\n",
      "이란 무엇인가\n",
      "?\n",
      "컴퓨터가 이해하는 텍스트란\n",
      "v = (1, 3, 4, ...., ..., )\n",
      "character encoding (word) embedding\n",
      "Source: https://velog.io/@goggling/유니코드와-UTF-이해하기\n",
      "Source: https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12\n",
      "Language Model ?\n",
      "이란 무엇인가\n",
      "= Next Token Prediction\n",
      "맥락의 이해\n",
      "going go eating\n",
      "\b\n",
      "Language Model ?\n",
      "이란 무엇인가\n",
      "Neural Language Modeling\n",
      "mat\n",
      "Source : https://lena-voita.github.io/nlp_course/language_modeling.html\n",
      "\b\n",
      "Language Model ?\n",
      "이란 무엇인가\n",
      "Neural Language Modeling in Auto-Regressive manner\n",
      "Source : https://lena-voita.github.io/nlp_course/language_modeling.html\n",
      "\b\n",
      "LLM\n",
      "이제는 대 의 시대\n",
      "LLM !\n",
      "모든 문제를 으로 해결해보자\n",
      "\b\n",
      "Large Language Model\n",
      "Neural Scaling Law\n",
      "• GPT-2\n",
      "모델 이후부터 학습 규모를 키우면 성능이 올라갈거라는 믿음이 생김\n",
      "• Size of the model : (Transformer )\n",
      "모델의 파라미터 수 를 기반으로\n",
      "• Size of the training dataset :\n",
      "학습하는 데이터의 수\n",
      "• , .\n",
      "단 모델과 데이터의 크기가 커지면 학습에 필요한 비용도 증가함\n",
      "• GPT-4 $63M (unofficial)\n",
      "가 약 정도 들었다고 알려짐\n",
      "•\n",
      "A100 x 2500 x 100 days\n",
      "\b\n",
      "Large Language Model\n",
      "Neural Scaling Law\n",
      "\b\n",
      "Large Language Model\n",
      "Emergent Ability\n",
      "•\n",
      "파라미터 수가 특정 개수를 넘어가면 그 때\n",
      "성능이 확 증가하는 구간이 존재함\n",
      "• LLM \"Emergent abillity\"\n",
      "이것을 의 라고\n",
      "정의함\n",
      "•\n",
      "더 많은 경험을 할 수록 기존에 하지 못하던\n",
      "새로운 인사이트를 이끌어 내는 느낌\n",
      "Source : https://arxiv.org/abs/2304.15004\n",
      "Large Language Model\n",
      "as Foundation Model\n",
      "• CV, NLP, RS LLM task\n",
      "등의 모든 분야에서 큰 하나를 가지고 여러 를 모두 수행하기 시\n",
      "작함\n",
      "• ChatGPT-4o-with-canvas\n",
      "대표적인 예로 같은 서비스들이 있음\n",
      "• LLM + Multi-modal approach LMM(Large Multi-Model)\n",
      "로 로 불리우며 지금은 대부\n",
      "Vision & Language\n",
      "분의 모델이 만을 담당하고 있음\n",
      "•\n",
      "대부분의 데이터가 이미지 또는 텍스트로 변환되며 이를 처리할 수 있으면 대다수의 정보\n",
      "처리가 가능\n",
      "\b\n",
      "Large Language Model\n",
      "as Foundation Model\n",
      "• But, LLM . ( + )\n",
      "은 학습하는데 비용이 너무 많이 든다는 문제가 있음 데이터 인프라\n",
      "• Also, Open-domain\n",
      "의 상황에서 모든 응답에 높은 퀄리티의 답변을 생성하는 것에는\n",
      ".\n",
      "무리가 있음\n",
      "• , Google Deepmind Chinchilla ,\n",
      "그리고 의 등장 이후 더 적은 파라미터로 높은 성능을\n",
      ".\n",
      "내는 모델들이 등장하기 시작함\n",
      "\b\n",
      "Small Language Model\n",
      "LLM\n",
      "은 너무 비용이 많이 든다\n",
      "\b\n",
      "Small Language Model\n",
      "LLM\n",
      "은 너무 비용이 많이 든다\n",
      "• Emergent ability\n",
      "의 등장으로 인해 큰 사이즈 모델이 각광받기 시작했으나 엄청난 비용이\n",
      "LLM\n",
      "발생하기 때문에 모든 회사들이 을 계속 쓰기는 힘듬\n",
      "• Pretrained LM , LM\n",
      "이전에 을 용도에 맞게 사용했듯 가성비 있는 이 필요해짐\n",
      "• < 13B\n",
      "현재는 많은 실험을 통해서 정도의 모델들을 주로 사용함\n",
      "\b\n",
      "Small Language Model\n",
      "open-source Language Model\n",
      "의 등장\n",
      "• Meta( Facebook) AI Research , Metaverse AI\n",
      "구 는 원래 의 강자였으나 에 올인하느라\n",
      "Follow-up .\n",
      "이 좀 늦어짐\n",
      "• GPT-3 .\n",
      "부터 모델이 공개가 되지 않음에 따라 영업비밀들이 점점 늘어나기 시작\n",
      "• openAI vs Google\n",
      "의 구도로 경쟁\n",
      "• Meta open-source LM\n",
      "는 새로운 돌파구로 을 공개함\n",
      "• OPT(Open Pretrained Transformer)\n",
      "를 무료로 올림\n",
      "• 125M ~ 175B\n",
      "까지 다양한 사이즈의 모델을 공개\n",
      "\b\n",
      "Small Language Model\n",
      "Llama(Large Language Model Meta AI)\n",
      "• 2023 2 Meta open-source LM Llama\n",
      "년 월 가 아주 뛰어난 를 발표\n",
      "• 7B, 13B, 33B, 65B 4\n",
      "개의 모델을 공개\n",
      "• 7B, 13B 1T token , 33B, 65B 1.4T token\n",
      "는 을 사용했고 는 을 학습에 사용함\n",
      "• (?)\n",
      "이전까지 연구되어온 최신 기법들을 모아서 엄청난 연구비를 대신 써줌\n",
      "\b\n",
      "(c) 2025. codingiscoffee Co. all rights reserved.\n",
      "Llama\n",
      "의 시대\n",
      "Llama2\n",
      "• 2023 7 Llama Llama2\n",
      "년 월 의 대흥행에 힘입어 이어서 를 발표\n",
      "• 7B, 13B, 70B 3 Chat-model\n",
      "개의 사이즈로 개발했으며 도 추가로 개발\n",
      "(Llama2-7B, Llama2-Chat-7B, ...)\n",
      "• 2T token , 4096 tokens\n",
      "모든 모델 을 사용했고 입력길이는\n",
      "• Llama ,\n",
      "대비 성능도 모두 증가했고 학습에 사용한 기법과 데이터가 바뀜\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama vs Llama2\n",
      "Source : https://Llama-2.ai/Llama-2-model-details/\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama2\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama3\n",
      "• Llama, Llama2 2024 4 Llama3\n",
      "가 완전히 자리를 잡자 년 월 도 발표\n",
      "• 8B, 70B 2\n",
      "가지 모델을 발표\n",
      "• 15T+ token , 8192 tokens\n",
      "모든 모델 을 학습했고 입력길이는\n",
      "• Llama2 ,\n",
      "에서 학습한 데이터의 크기를 증가시키고 대세에 따라 더 많은 입력을 받을 수\n",
      "있게 변경\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama3\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama3.1\n",
      "• Llama3 , 24 7 3.1\n",
      "가 엄청나게 큰 영향을 주면서 자리를 잡자 이어 년 월에 을 발표\n",
      "• 8B, 70B, 405B 3 . (405B )\n",
      "가지 모델을 발표함 는 오픈소스 중 가장 큰 크기의 모델\n",
      "• 15T+ token , 128K tokens\n",
      "모든 모델 을 학습했고 입력길이는\n",
      "• Llama3 task\n",
      "보다 더 많은 언어에 대한 공식지원과 더 다양한 에 대한 성능을 향상시킴\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama3.1\n",
      "• Llama 3.1 .\n",
      "까지 적용된 기술들은 다음과 같다\n",
      "• RoPE(Rotary Positional Embedding)\n",
      "를 사용하여 긴 컨텍스트도 잘 이해할 수 있게 학습\n",
      "• 128K tokens\n",
      "한번에 최대 입력 가능\n",
      "•\n",
      "trained over 15B tokens\n",
      "• Grouped-Query Attention\n",
      "적용\n",
      "• RLHF safety guide\n",
      "로 모델의 응답이 를 준수하도록 학습함\n",
      "• 16,000 H100 cluster\n",
      "개의 를 사용함\n",
      "\b\n",
      "(c) 2025. codingiscoffee Co. all rights reserved.\n",
      "2. Llama 3.2 Large Multi-Model(LMM)\n",
      "를 통해 보는 의 이해\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama3.2\n",
      "• 24 7 3.1 (?) 24 9 3.2\n",
      "년 월에 을 발표한지 얼마안되어 갑자기 년 월에 를 발표\n",
      "• 2 Lightweight Multimodal\n",
      "가지의 큰 키워드로 과 을 잡고 모델을 발표\n",
      "• Lightweight 1B, 3B Multimodal 11B, 90B\n",
      "용 를 용 사이즈를 출시\n",
      "• 3.1 15T+ token , 128K tokens\n",
      "과 동일하게 모든 모델 을 학습했고 입력길이는\n",
      "• 3.1 AWS, Nvidia\n",
      "때보다 좀 더 서비스 지향적으로 바뀌었으며 등 다양한 회사들과 협업\n",
      "\b\n",
      "(c) 2025. codingiscoffee Co. all rights reserved.\n",
      "(c) 2025. codingiscoffee Co. all rights reserved.\n",
      "Llama\n",
      "의 시대\n",
      "Llama3.2\n",
      "• Llama 3.2 “on-device AI” “Large Multi-Model”\n",
      "가 발표되던 시점의 주요 키워드는 와 이\n",
      ".\n",
      "었음\n",
      "• ,\n",
      "진짜 킬러 서비스가 등장하려면 헤비하지 않게도 일상 생활에 도움이 되는 여러 디바이스들\n",
      "(e.g. , , )\n",
      "스마트폰 전기차 스마트 가전 등 에서 작동해야 함\n",
      "> on-device AI\n",
      "• , .\n",
      "입력이 텍스트가 아닌 이미지 오디오가 되면 훨씬 더 확장성이 커짐\n",
      "> LLM .\n",
      "이미지 입력을 받을 수 있는 이 생기면 사용성이 증가함\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama3.2\n",
      "• Lightweight model Llama 3.1 8B, 70B Knowledge Distillation\n",
      "들은 의 모델을 하여\n",
      "학습시킴\n",
      "• SpinQuant QLoRA 52~60% ,\n",
      "와 를 적용하여 모델 크기는 감소시키고 추론 속도는\n",
      "2.4~2.6\n",
      "배 향상시킴\n",
      "• Llama 3.1\n",
      "보다 더 많은 안전성 테스트를 진행함\n",
      "(advarsarial prompting red teaming )\n",
      "에 대한 도 수행\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama3.2 - Lightweight model\n",
      "• Edge-device ( , ) inference\n",
      "스마트폰 스마트카 등 에서 자체적으로 를 할 수 있으려면 적은 리소스로\n",
      ".\n",
      "도 충분한 성능을 내야함\n",
      "• .\n",
      "그러기 위해선 다양한 경량화 기법들을 적용하면서도 성능을 유지시키기 위한 노력들이 필요함\n",
      "1. Model quantization\n",
      "2. PyTorch’s ExecuTorch\n",
      "3. Quantization-Aware Training(QAT)\n",
      "4. Parameter Efficient Fine-Tuning(PEFT)\n",
      "5. Post Training\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama3.2 - 1. Model Quantization\n",
      "• Model Quantization\n",
      "은 같은 크기를 가지는 모델을 가볍게 만들기 위한 기법들로 파라미\n",
      "터의 수는 유지하되 사용하는 메모리량을 줄이는 것에 목적이 있음\n",
      "• float32 bfloat16\n",
      "가장 간단한 방법은 기존에 로 표현되던 실수형 데이터 타입을 으로 변경\n",
      ". .\n",
      "하는 것 말그대로 메모리 사용량이 절반으로 줄어듬\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama3.2 - 1. Model Quantization\n",
      "• Knowledge Distillation .\n",
      "다른 방식으로는 큰 모델의 학습 결과를 모방하는 이 있음 이 방\n",
      "(logit) (logit) .\n",
      "식은 작은 모델의 출력 이 큰 모델의 출력 과 같아지게 만드는 것에 있음\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama3.2 - 2. ExecuTorch\n",
      "• framework Pytorch on-device AI inference\n",
      "학습에 주로 사용하는 인 에서 나온 를 위한\n",
      "ExecuTorch framework edge device CPU inference\n",
      "라는 를 사용하면 의 에서 빠르게 가\n",
      "가능\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama3.2 - 3. QAT\n",
      "• fake quantization quantization\n",
      "학습 과정에서 을 하여 마치 을 수행하는 것처럼 학습을\n",
      "INT8 FP32\n",
      "진행하면 데이터 타입을 사용하면서도 에 가까운 성능을 낼 수 있는 기법\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama3.2 - 4. PEFT\n",
      "• Supervised Fine-Tuning , weight\n",
      "을 수행할 때 모든 를 업데이트하게 되면 많은 비용이\n",
      "발생하므로 이를 효율적으로 하기 위해 업데이트되는 내용만 따로 추가적인 구조에 학습하\n",
      "는 것으로 적은 리소스로 학습한 효과를 내는 학습 방법\n",
      "• LoRA, QLoRA QLoRA quantization\n",
      "가장 유명한 기법엔 가 있는데 가 이 적용된 기법이라\n",
      "더욱 효율적인 학습이 가능\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama3.2 - 5. Post Training\n",
      "• Pre-training Llama 3.2 task\n",
      "일반적인 문맥과 정보를 학습하는 단계를 거친 를 특정 나\n",
      "유저의 선호도에 맞는 답변을 생성하기 위해 수행하는 단계\n",
      "•\n",
      "Supervised Fine-Tuning(SFT), Direct Preference Optimization(DPO), Rejection\n",
      "Sampling(RS) .\n",
      "가 주로 사용됨\n",
      "• Llama 3.2 Meta\n",
      "이러한 방식을 사용하면 를 에서 원하는 방향대로 생성되는 답변의 퀄리티\n",
      ". (Alignment)\n",
      "를 조절할 수 있음\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama3.2 - Vision\n",
      "• LLM Vision Context , multi-modality .\n",
      "에 를 추가하여 를 추가함\n",
      "• Vision Reasoning , .\n",
      "이 가능해지면 더욱 많은 서비스를 사용할 수 있음\n",
      "> , vs\n",
      "길을 물어볼 때 말로 하는 것 지도를 제공\n",
      "> , vs\n",
      "코드 오류가 발생했을 때 텍스트로 설명하는 것 캡처 사진 제공\n",
      "• Audio Context .\n",
      "같은 맥락으로 로 추가하는 식으로 계속해서 확장 가능함\n",
      "\b\n",
      "(c) 2025. codingiscoffee Co. all rights reserved.\n",
      "Source: https://openai.com/product\n",
      "Llama\n",
      "의 시대\n",
      "Llama3.2 - Vision\n",
      "[‘이’, ‘나비는’, ’어떤’, ‘단계에’, ‘있나요?’]\n",
      "• multi-modal input layer\n",
      "의 기본 학습 원리는\n",
      ".\n",
      "를 확장하는 것에 있음\n",
      "• Image Encoder(ViT-\n",
      "기존에 널리 사용되는\n",
      "H/14) Image Language\n",
      "를 사용하여 를\n",
      "Model .\n",
      "에 추가 학습함\n",
      "• LLM cross-attention layer\n",
      "의 에서\n",
      "matching image token\n",
      "되어 들어오는 과\n",
      "text token .\n",
      "사이의 관련성을 학습함\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama3.2 - Audio\n",
      "[‘이’, ‘나비는’, ’어떤’, ‘단계에’, ‘있나요?’]\n",
      "• multi-modal input layer\n",
      "의 기본 학습 원리는\n",
      ".\n",
      "를 확장하는 것에 있음\n",
      "• Audio\n",
      "기존에 널리 사용되는\n",
      "Encoder(Whisper-Large)\n",
      "를 사용하여\n",
      "Audio Waveform Language Model\n",
      "를 에 추\n",
      ".\n",
      "가 학습함\n",
      "• LLM cross-attention layer\n",
      "의 에서\n",
      "matching Audio segment\n",
      "되어 들어오는\n",
      "token text token .\n",
      "과 사이의 관련성을 학습함\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Llama3.3 ( …)\n",
      "이젠 갑자기 나와도 놀랍지 않음\n",
      "• 24 12 6 Llama 3.3\n",
      "갑자기 년 월 일에 을 발표\n",
      "• Multilingual 70B .\n",
      "의 키워드로 모델 하나만 발표\n",
      "• 3.1 70B , benchmark\n",
      "의 의 성능을 향상 시킨 것으로 보이며 실제 여러 기준으로 어마어마\n",
      ". ( SOTA)\n",
      "한 성능 개선이 있는건 아님 하지만 당연히\n",
      "• Multilingual .\n",
      "글로벌한 사용성을 기준으로 파트의 성능이 비약적으로 상승함\n",
      "\b\n",
      "Llama\n",
      "의 시대\n",
      "Now available\n",
      "\b\n",
      "Source: https://www.Llama.com/Llama-downloads\n",
      "open-source LLMs\n",
      "그 외의\n",
      "\b\n",
      "Source: https://openai.com/product\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "\b\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Retrieval Augmented Generation\n",
      "Source : https://python.langchain.com/docs/tutorials/rag/\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Retrieval System + LLM\n",
      "• (Question) (Knowledge)\n",
      "사용자의 질문 에 대한 답변을 미리 저장해둔 정보들 에서 찾고\n",
      "(Retrieve) (Augmented) LLM (Generate) .\n",
      "찾은 내용을 합쳐서 으로 답변을 생성 하는 방법\n",
      "• Question , Knowledge , Retrieve , Generation\n",
      "도 중요하고 도 중요하고 도 잘해야하고 도\n",
      ".\n",
      "잘되어야 성능이 잘 나온다\n",
      "• LLM Question, Generation\n",
      "의 발전으로 은 어느정도 신경을 덜 써도 성능이 잘 나오는 편이\n",
      ", Knowledge Retrieve challenging .\n",
      "지만 구축과 를 잘하는 것은 큰 이 있다\n",
      "\b\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Retrieval System + LLM\n",
      "• Knowledge .\n",
      "는 사용자의 질문에 대해 정확한 답변을 제공하기 위한 용도로 구축된다\n",
      "• Knowledge Database ,\n",
      "를 구축할 때는 를 사용하며 어떤 정보를 처리할것이냐에 따라\n",
      "RDBMS or Vector Store .\n",
      "를 사용한다\n",
      "• RDBMS , Text2SQL .\n",
      "는 테이블 데이터를 다루는 용도로 사용되며 을 거쳐서 사용된다\n",
      "• Vector Store / , /\n",
      "는 이미지 텍스트를 주로 다루는 용도로 사용되며 이미지 텍스트를\n",
      "embedding vector .\n",
      "로 변환하여 저장하는 용도로 사용된다\n",
      "\b\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Retrieval System + LLM\n",
      "• Knowledge Database Knowledge Base(KB) , KB\n",
      "이렇게 가 저장된 를 합쳐서 라고 하며\n",
      "Retrieval . , .\n",
      "를 잘 구축하여야 의 성능이 올라감 즉 검색이 될 자료가 있어야 함\n",
      "• . (= )\n",
      "사용자 질문에 도움이 되는 정보를 찾는 과정도 중요함 질문과 유사성이 높은 관련이 높은\n",
      "Retrieval .\n",
      "자료를 찾는 것이 의 성능에 영향을 미침\n",
      "• = Ranking\n",
      "잘 찾는 것\n",
      "\b\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Retrieval System + LLM\n",
      "• LLM RAG LLM pre-training .\n",
      "이 에서 답변을 잘 생성하기 위해선 의 성능이 중요하다\n",
      "• , token pre-training . (SOTA)\n",
      "주로 많은 을 학습시킨 모델의 성능이 높은 편이다\n",
      "• , LLM knowledge-cutoff , .\n",
      "하지만 은 문제가 있어 항상 최신 데이터를 학습할 수 없다\n",
      "( LLM 23 12 cutoff )\n",
      "대부분의 주요 들이 년 월에 되어 있음\n",
      "• Knowledge Base .\n",
      "이러한 점을 극복할 수 있는게 에 최신 데이터를 추가하는 방법이다\n",
      "\b\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Langchain RAG\n",
      "에서 를 구축하는 순서\n",
      "Source : https://developer.nvidia.com/ko-kr/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Langchain RAG\n",
      "에서 를 구축하는 순서\n",
      "1. Document Loading\n",
      "2. Text Splitting\n",
      "3. Embedding\n",
      "4. Vector Store (= Indexing)\n",
      "5. Retrieving\n",
      "6. Generating\n",
      "\b\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Langchain RAG\n",
      "에서 를 구축하는 순서\n",
      "1. Document Loading • KB .\n",
      "에서 데이터를 가져오는 작업\n",
      "2. Text Splitting\n",
      "•\n",
      "파일의 종류에 따라\n",
      "3. Embedding\n",
      "langchain_community.document_load\n",
      "ers .\n",
      "에 존재하는 구현체가 나뉨\n",
      "4. Vector Store (= Indexing)\n",
      "e.g. PyPDFLoader, JsonOutputParser,\n",
      "5. Retrieving\n",
      "…\n",
      "6. Generating\n",
      "\b\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Langchain RAG\n",
      "에서 를 구축하는 순서\n",
      "1. Document Loading • chunk .\n",
      "가져온 텍스트를 로 나누는 작업\n",
      "2. Text Splitting\n",
      "•\n",
      "어떤 단위로 나누냐에 따라 여러 구현체를\n",
      "3. Embedding\n",
      ".\n",
      "사용함\n",
      "4. Vector Store (= Indexing)\n",
      "e.g. RecursiveCharacterTextSplitter,\n",
      "SemanticChunker, …\n",
      "5. Retrieving\n",
      "6. Generating\n",
      "\b\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Langchain RAG\n",
      "에서 를 구축하는 순서\n",
      "1. Document Loading •\n",
      "가져온 텍스트 데이터를 벡터 공간에 이식하는\n",
      ".\n",
      "작업\n",
      "2. Text Splitting\n",
      "3. Embedding • -\n",
      "이 작업을 거쳐야 질문 지식 간의 유사성 계산\n",
      ".\n",
      "이 가능함\n",
      "4. Vector Store (= Indexing)\n",
      "5. Retrieving\n",
      "• embedding pre-trained\n",
      "은 주로\n",
      "embedding model .\n",
      "을 사용함\n",
      "6. Generating\n",
      "e.g. OpenAIEmbeddings,\n",
      "HuggingFaceEmbeddings\n",
      "…\n",
      "\b\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Langchain RAG\n",
      "에서 를 구축하는 순서\n",
      "1. Document Loading • embedding vector\n",
      "저장한 들을 빠르게 계\n",
      "Index .\n",
      "산하기 위해서 를 만드는 작업\n",
      "2. Text Splitting\n",
      "3. Embedding\n",
      "• Index ,\n",
      "를 만들어두면 매번 검색할 때 전탐색\n",
      "을 수행할 필요가 없이 빠르게 필요한 영역\n",
      "4. Vector Store (= Indexing)\n",
      ".\n",
      "만 탐색이 가능\n",
      "5. Retrieving\n",
      "• Indexing\n",
      "어떤 방식을 쓰느냐에 따라 다양\n",
      "6. Generating\n",
      ".\n",
      "한 구현체가 존재함\n",
      "e.g. Chroma, FAISS, Milvus, …\n",
      "\b\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Langchain RAG\n",
      "에서 를 구축하는 순서\n",
      "1. Document Loading • embedding\n",
      "사용자의 입력과 연관있는\n",
      "vector .\n",
      "를 찾는 과정\n",
      "2. Text Splitting\n",
      "3. Embedding\n",
      "• K vector\n",
      "요청한 개의 유사성이 높은 를 찾고\n",
      "vector .\n",
      "해당 의 원본 텍스트를 반환\n",
      "4. Vector Store (= Indexing)\n",
      "5. Retrieving\n",
      "• Embedding quality\n",
      "에 따라 성능이 큰\n",
      "!\n",
      "영향을 받음\n",
      "6. Generating\n",
      "\b\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Langchain RAG\n",
      "에서 를 구축하는 순서\n",
      "1. Document Loading •\n",
      "추가로 찾은 텍스트와 원본 질문을 함께\n",
      "System Prompt\n",
      "미리 디자인해둔 의 형태로\n",
      "2. Text Splitting\n",
      "LLM .\n",
      "에 입력으로 제공\n",
      "3. Embedding\n",
      "• Retrieval ,\n",
      "된 문서가 많을 수록 문서의 길이가\n",
      "4. Vector Store (= Indexing)\n",
      "input prompt\n",
      "길수록 가 길어지기 때문에 생성\n",
      ".\n",
      "되는 답변에 큰 영향을 줌\n",
      "5. Retrieving\n",
      "6. Generating\n",
      "+ LLM token length !\n",
      "사용하는 의 도 고려해야 함\n",
      "\b\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "RAG Evaluation Metric\n",
      "• RAG LLM\n",
      "를 사용해서 이 생성한 답변이 실제로 얼마나 좋은 답변인지를 평가하는 것은\n",
      ".\n",
      "매우 어려움\n",
      "( .)\n",
      "사람이 작성한 텍스트의 품질 평가도 쉽지 않음\n",
      "• ,\n",
      "주관적 해석이 많이 포함되고 도메인에 따라 당연히 중요도가 달라지지만 대세를 결정하\n",
      ".\n",
      "는 주요 지표들을 소개함\n",
      "• RAG LLM .\n",
      "를 잘 사용한 것과 이 답변을 그냥 잘 만든 것은 조금 다름\n",
      "“ ” .\n",
      "얼마나 정보를 잘 사용했느냐 를 판단하는 것이 관건\n",
      "\b\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "RAG Evaluation Metric\n",
      "• Ranking metric RAG\n",
      "기존에 정보 검색에서 많이 사용하는 들을 의 특성에 맞게 변형하여\n",
      ".\n",
      "사용함\n",
      "• NLP , BLEU, ROUGE-N\n",
      "기존에 에서 텍스트 생성을 평가하던 등의 점수는 잘 사용하지\n",
      ".\n",
      "않음\n",
      "( , token )\n",
      "왜냐면 생성된 이 같은지 다른지만 체크하는게 전혀 의미가 없어서\n",
      "• RAG ground truth .\n",
      "를 평가해야하기 때문에 의 영향을 많이 받을 수 밖에 없음\n",
      "\b\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Evaluation Metric 1. Context Precision\n",
      "• .\n",
      "수식의 의미는 검색한 문서 중에서 진짜로 관련된 문서가 차지하는 비율\n",
      "Source : https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision/\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Evaluation Metric 2. Context Recall\n",
      "• Retrieval .\n",
      "수식의 의미는 실제로 관련된 문서 중에 얼마나 많이 에 성공했는지\n",
      "Source : https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_recall/\n",
      "\b\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Evaluation Metric OS tool : Ragas\n",
      "Source : https://docs.ragas.io/en/stable/\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Evaluation Metric tool : Ragas\n",
      "• Ragas LLM .\n",
      "는 의 성능 평가를 쉽게 구현해줄 수 있는 오픈소스 라이브러리이다\n",
      "• Langchain , .\n",
      "과 굉장히 쉽게 호환이 되며 사용성이 편리하여 평가 지표 구현이 쉽다\n",
      "Source : https://docs.ragas.io/en/stable/getstarted/rag_evaluation/\n",
      "\b\n",
      "3. RAG\n",
      "에 대한 이해\n",
      "Evaluation Metric tool : Ragas\n",
      "• Ragas RAG , AI Agent, Natural Language\n",
      "에는 를 평가할 때 쓰이는 지표들뿐만 아니라\n",
      "Compression, SQL .\n",
      "등을 평가하는 지표들도 제공한다\n",
      "• Ground Truth .\n",
      "당연히 평가요소에서 가 핵심이다\n",
      "• Ground Truth .\n",
      "평가를 잘하기 위해서는 동의가 된 들이 필요하다\n",
      "• Ground Truth , .\n",
      "는 사람이 만드는게 정석이지만 간접적인 여러가지 추가 방법론들도 존재한다\n",
      "e.g. LLM-as-a-Judge\n",
      "\b\n",
      "4. LLM App\n",
      "구축시 고려할 사항들에 대한 이해\n",
      "\b\n",
      "1) LLM SLM ?\n",
      "과 중 어떤 모델이 좋을까\n",
      "LLM ?\n",
      "당연히 이 더 좋을까\n",
      "• LLM , .\n",
      "성능은 당연히 이 우수하지만 두 가지의 큰 측면에서 한계점이 있다\n",
      "•\n",
      "첫번째는 보안\n",
      "•\n",
      "두번째는 비용\n",
      "•\n",
      "세번째는 우리꺼가 갖고 싶음\n",
      "\b\n",
      "1) LLM SLM ?\n",
      "과 중 어떤 모델이 좋을까\n",
      "LLM ?\n",
      "당연히 이 더 좋을까\n",
      "•\n",
      "보안의 측면에서 회사들은 자사의 데이터를 다른 회사에게 넘겨주고 싶지 않아함\n",
      "( / )\n",
      "물론 아예 안되는 회사 기관들도 존재\n",
      "• LLM API\n",
      "은 라는 형태로 네트워크를 통해서 서버에서 동작하기 때문에 무조건 요청과 함께\n",
      "데이터가 넘어가야 함\n",
      "• Prompt Engineering\n",
      "레벨에서 해결할 수 있지만 사용성에서 한계가 있어 어려움\n",
      "\b\n",
      "1) LLM SLM ?\n",
      "과 중 어떤 모델이 좋을까\n",
      "LLM ?\n",
      "당연히 이 더 좋을까\n",
      "• , 2\n",
      "비용에서는 여전히 갑론을박이 많지만 대세에는 크게 가지 의견이 존재함\n",
      "1) LLM\n",
      "이 훨씬 비용이 저렴하다\n",
      "- SLM API call\n",
      "자체 개발에 드는 비용보다 당 비용을 지불하는게 더 합리적임\n",
      "- LLM\n",
      "생각보다 을 이용하여 서비스하는게 비용이 많이 들지 않음\n",
      "- LLM API\n",
      "비용이 점점 저렴해지고 있음\n",
      "\b\n",
      "1) LLM SLM ?\n",
      "과 중 어떤 모델이 좋을까\n",
      "LLM ?\n",
      "당연히 이 더 좋을까\n",
      "• , 2\n",
      "비용에서는 여전히 갑론을박이 많지만 대세에는 크게 가지 의견이 존재함\n",
      "2) SLM\n",
      "이 훨씬 비용이 저렴하다\n",
      "- , LLM API\n",
      "네이버 카카오처럼 엄청나게 많은 사용량이 있는 회사에서 당 비용 지불은 너\n",
      "무 많은 지출이 필요함\n",
      "- SLM\n",
      "생각보다 을 학습시켜서 서비스를 하는게 크게 어렵지 않음\n",
      "- ( + ) ,\n",
      "초기 투자 비용 인력 인프라 이 많이 들긴하지만 서비스를 할수록 차이가 큼\n",
      "\b\n",
      "1) LLM SLM ?\n",
      "과 중 어떤 모델이 좋을까\n",
      "LLM ?\n",
      "당연히 이 더 좋을까\n",
      "• Hybrid!\n",
      "대세는\n",
      "• , LLM\n",
      "앞선 특징을 모두 보면 알 수있듯 작은 서비스에는 을\n",
      "SLM\n",
      "큰 서비스에는 을 쓰는 것이\n",
      ".\n",
      "비용 측면에서 효율적이라고 볼 수 있다\n",
      "• .. ..?\n",
      "그럼 작은 서비스와 큰 서비스의 기준은\n",
      "• LLM ?\n",
      "에서 보안 이슈는 어떻게 해결하는가\n",
      "\b\n",
      "2) SFT vs RAG\n",
      "LM ...?\n",
      "의 답변이 마음에 들지 않을때\n",
      "• SFT(Supervised Fine Tuning) ,\n",
      "은 데이터를 모델에 학습시켜서 내가 원하는 답변을 잘\n",
      "만들 수 있게 하는 방식\n",
      "• RAG(Retrieve Augmented Generation)\n",
      "은 데이터를 잘 정리해놓은 지식 창고\n",
      "(Knowledge Base, VectorDB)\n",
      "주로 에서 답변과 연관된 지식을 찾아서 답변을 생성하는\n",
      "방식\n",
      "• ,\n",
      "두 가지를 모두 사용하면 제일 효과적이겠지만 장단점이 존재함\n",
      "\b\n",
      "2) SFT vs RAG\n",
      "LM ...?\n",
      "의 답변이 마음에 들지 않을때\n",
      "• SFT .\n",
      "를 하려면 우선 학습 데이터를 구축하는 것부터 시작해야 함\n",
      "• task\n",
      "명확한 에 대한 정의와 그에 대한 데이터의\n",
      ".\n",
      "양이 충분해야 함\n",
      "• Instruction Tuning\n",
      "이런 작업은 주로 이라는\n",
      ".\n",
      "이름으로 수행됨\n",
      "\b\n",
      "2) SFT vs RAG\n",
      "LM ...?\n",
      "의 답변이 마음에 들지 않을때\n",
      "• , LM SFT\n",
      "하지만 의 크기가 클수록 는 엄청나게 많은 비용이 발생함\n",
      "(GPU ..)\n",
      "가 곧 시간이자 돈\n",
      "• SFT overfitting ,\n",
      "를 한다고 해서 무조건 성능이 증가하는게 아니라서 이슈도 있고 답변 생\n",
      ". (domain-dependent)\n",
      "성의 성능 측정 자체가 굉장히 주관적임\n",
      "• RLHF DPO\n",
      "그래서 보통은 추가적으로 나 같은 강화학습 방법도 함께 사용하여 아예 성능\n",
      ".\n",
      "을 끌어올리기 위해 최선을 다함\n",
      "\b\n",
      "2) SFT vs RAG\n",
      "LM ...?\n",
      "의 답변이 마음에 들지 않을때\n",
      "• RAG LLM\n",
      "는 의 입력 프롬프트에 사용할 수 있는 적절한 지식을 자동으로 찾아서 생성\n",
      ".\n",
      "답변의 퀄리티를 높이는 방식\n",
      "•\n",
      "답변을 원하는 좋은 지식을 잘 정리해서\n",
      ". (Knowledge Base )\n",
      "저장해두어야 함 구축\n",
      "• /\n",
      "학습 대비 시간 비용 효율적이어서 요즘\n",
      "많이 사용되고 있음\n",
      "\b\n",
      "2) SFT vs RAG\n",
      "LM ...?\n",
      "의 답변이 마음에 들지 않을때\n",
      "• RAG KB ,\n",
      "는 그냥 에 답변을 때려넣는다고 해결이 되는게 아니라\n",
      "\" \" .\n",
      "관련도가 높은 문서를 잘 찾아야 해결됨\n",
      "• .\n",
      "잘 찾는게 굉장히 어려움\n",
      "• SFT RAG ,\n",
      "처럼 역시나 를 위한 데이터셋 구축이 어렵고 도메인\n",
      ".\n",
      "마다 특성이 모두 달라서 도메인 전문가가 꼭 필요함\n",
      "• ,\n",
      "대신에 실시간으로 데이터를 추가할 수 있고 학습보단 필요한\n",
      "데이터셋의 양이 적은 편\n",
      "\b\n",
      "2) SFT vs RAG\n",
      "당연히 둘 다 할 수 있는게 제일 좋음\n",
      "• Hybrid!\n",
      "대세는\n",
      "• , RAG\n",
      "앞선 특징을 모두 보면 알 수있듯 시작에는 를\n",
      "SFT\n",
      "데이터가 쌓여감에 따라 를 쓰는 것이\n",
      ".\n",
      "비용 측면에서 효율적이라고 볼 수 있다\n",
      "• RAG ...?\n",
      "그럼 항상 시작할 땐 부터\n",
      "• RAG SFT ?\n",
      "나 를 하면 무조건 답변의 성능이 좋아지나\n",
      "\b\n",
      "3) Langchain vs LlamaIndex\n",
      "framework LLM service ?\n",
      "어떤 가 개발에 적합할까\n",
      "VS\n",
      "\b\n",
      "3) Langchain vs LlamaIndex\n",
      "framework LLM service ?\n",
      "어떤 가 개발에 적합할까\n",
      "• Langchain!\n",
      "대세는\n",
      "• app Langchain , LLM\n",
      "현재 가장 많은 들이 기반으로 작성되고 있으며 사용자들이 쉽게 을\n",
      ".\n",
      "구축할 수 있는 많은 기능들을 제공함\n",
      "• ,\n",
      "쉽게 구현할 수 있으나 다양한 데이터베이스를 고려하고 그에 맞는 쿼리나 메모리 관리를\n",
      ".\n",
      "효율적으로 하기엔 기능이 부족함\n",
      "\b\n",
      "3) Langchain vs LlamaIndex\n",
      "framework LLM service ?\n",
      "어떤 가 개발에 적합할까\n",
      "• LlamaIndex\n",
      "는 이러한 점들을 보완할 수 있는 다양한 데이터 소스와 고급 인덱싱 기법들\n",
      ".\n",
      "을 지원함\n",
      "• Langchain ,\n",
      "대신에 대비 개발자 커뮤니티가 적으며 아무래도 처음에 공부할 때\n",
      "Langchain learning curve .\n",
      "대비 가 큰 편이라서 접근이 쉽지 않음\n",
      "• LlamaIndex\n",
      "대용량 트래픽을 고려해야하는 서비스의 경우 를 기반으로 구현하는 것이\n",
      ".\n",
      "장기적으로 봤을 때 서비스 구축에 도움이 됨\n",
      "\b\n",
      "4) Ollama\n",
      "LLM\n",
      "개인용 컴퓨터에서 을 쉽게 실행할 수 있는 플랫폼\n",
      "• LLM/SLM ( )\n",
      "오픈소스이면서 에 접근하기 쉽게 제작된 프로그램 이자 플랫폼\n",
      "• .\n",
      "다운로드 받아서 바로 모델들을 직접 돌려볼 수 있음\n",
      "• Rust .\n",
      "로 구현되어 있어 굉장히 빠른 속도를 자랑함\n",
      "• ,\n",
      "사용자들이 쉽게 쓸 수 있게 깔끔한 최소한의 기능들을 제공하며 최신 모델들이 빠르게\n",
      ".\n",
      "업데이트 됨\n",
      "\b\n",
      "5) Llama Stack\n",
      "Llama .\n",
      "모델을 이용하여 서비스를 빠르게 구현해볼 수 있는 플랫폼\n",
      "• Llama 3.2 .\n",
      "가 공개될 당시에 같이 공개됨\n",
      "• Llama Llama\n",
      "가 본격적으로 하나의 상품처럼 인식되며 여러 클라우드 업체들과 협업하여\n",
      ".\n",
      "를 이용해서 개발을 쉽게 할 수 있게 출시됨\n",
      "• , .\n",
      "아직까지는 사용자가 거의 없어 사용해볼 경우조차 많지 않음\n",
      "• , Llama\n",
      "하지만 모델이 가지는 파워가 큰만큼 연동만 잘된다면 노코드 구현 서비스처럼\n",
      ".\n",
      "확장될 가능성이 있음\n",
      "\b\n",
      "5) Llama Stack\n",
      "Llama .\n",
      "모델을 이용하여 서비스를 빠르게 구현해볼 수 있는 플랫폼\n",
      "\b\n",
      "End of Slides\n",
      "\b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def get_pdf_files(files):\n",
    "  pdf_files = checkExtension('pdf', files)\n",
    "\n",
    "  print(\"감지된 pdf 자료: \", end=\"\")\n",
    "  print(pdf_files)\n",
    "\n",
    "  # 실제로 파일 읽어서 배열에 집어넣기\n",
    "  pdf_docs = []\n",
    "\n",
    "  for file in pdf_files:\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(file) as pdf:\n",
    "      for page in pdf.pages:\n",
    "        tmp = page.extract_text()\n",
    "        if tmp.find(\"(c) 2025. codingiscoffee Co. all rights reserved.\"):\n",
    "          tmp = tmp.replace(\"(c) 2025. codingiscoffee Co. all rights reserved.\", \"\")\n",
    "        text += tmp + \"\\n\"\n",
    "    pdf_docs.append(text)\n",
    "        \n",
    "  return pdf_docs\n",
    "\n",
    "for a in get_pdf_files(all_files):\n",
    "  print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dff3aa",
   "metadata": {},
   "source": [
    "## 3. 임베딩/벡터스토어 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b405577f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감지된 markdown 자료: ['./docs/testmd.md', './docs/testtest.md', './docs/md/testmdmd.md']\n",
      "파일 읽음: ./docs/testmd.md\n",
      "파일 읽음: ./docs/testtest.md\n",
      "파일 읽음: ./docs/md/testmdmd.md\n",
      "감지된 pdf 자료: ['./docs/pdf/AI Trend with LLM.pdf']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "docs = get_md_files(all_files) + get_pdf_files(all_files)\n",
    "\n",
    "# 문서 청크로 나누기\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "splits = splitter.create_documents(docs)\n",
    "\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vstore = FAISS.from_documents(splits, emb)\n",
    "retriever = vstore.as_retriever(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "da022fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='당신은 문서 기반 QA 비서입니다.\\n질문: {question}\\n검색 컨텍스트:\\n{context}\\n요청: 위 컨텍스트만 근거로 간결히 한국어로 답하세요.\\n'), additional_kwargs={})]\n",
      "RAG의 순서는 다음과 같습니다:\n",
      "\n",
      "1. Document Loading\n",
      "2. Text Splitting\n",
      "3. Embedding\n",
      "4. Vector Store (= Indexing)\n",
      "5. Retrieving\n",
      "6. Generating\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"당신은 문서 기반 QA 비서입니다.\n",
    "질문: {question}\n",
    "검색 컨텍스트:\n",
    "{context}\n",
    "요청: 위 컨텍스트만 근거로 간결히 한국어로 답하세요.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(prompt)\n",
    "print(rag_chain.invoke(\"rag의 순서는?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
